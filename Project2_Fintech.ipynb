{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkiLIpdlHw7PYFVnmfDwTV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86cfdcc89e954677b4c26516ebc25731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04d0fc28e7154c63bf4c6eb6c83ad27d",
              "IPY_MODEL_e99f8f729686496697cfa443b6c01d2d",
              "IPY_MODEL_20efbcadf0f54b358e3b239078d35763"
            ],
            "layout": "IPY_MODEL_934b9c4ff6194f2b96b266297baf6f7e"
          }
        },
        "04d0fc28e7154c63bf4c6eb6c83ad27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80cd35fe6f5747e0b9f61df1a9dca2dc",
            "placeholder": "​",
            "style": "IPY_MODEL_bca132f1ca25427c87ff65b4e1275aed",
            "value": "Downloading: 100%"
          }
        },
        "e99f8f729686496697cfa443b6c01d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a8a65951e546b2ba3acf1c019d4a0b",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54afbaace4b04c0dac71b08f995f99ac",
            "value": 213450
          }
        },
        "20efbcadf0f54b358e3b239078d35763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d28db5e740a4cafafd26973fb21e2d7",
            "placeholder": "​",
            "style": "IPY_MODEL_b3e2be223069448b96b71405211c8f35",
            "value": " 213k/213k [00:00&lt;00:00, 299kB/s]"
          }
        },
        "934b9c4ff6194f2b96b266297baf6f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80cd35fe6f5747e0b9f61df1a9dca2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bca132f1ca25427c87ff65b4e1275aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6a8a65951e546b2ba3acf1c019d4a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54afbaace4b04c0dac71b08f995f99ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d28db5e740a4cafafd26973fb21e2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e2be223069448b96b71405211c8f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4e17e8722bc43a0bb18f82de8ab6498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c51cb983a0f4088962c53f7658a341d",
              "IPY_MODEL_e00aa54e94504d1392ed08dd60aa8a63",
              "IPY_MODEL_3389e5c809d7440da49233fb55452801"
            ],
            "layout": "IPY_MODEL_9129aabd8c6941f780ca9b064bcab931"
          }
        },
        "4c51cb983a0f4088962c53f7658a341d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_724f205e7c9d41a181076d57a514ca42",
            "placeholder": "​",
            "style": "IPY_MODEL_5864f48a1208422794882c0299a7dda6",
            "value": "Downloading: 100%"
          }
        },
        "e00aa54e94504d1392ed08dd60aa8a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5beda64b4365408080c3c4fccadfb6f4",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b2eb59ea1654c1eaf12d89c7609508c",
            "value": 29
          }
        },
        "3389e5c809d7440da49233fb55452801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4576d5977bde40f5bf979c450868b974",
            "placeholder": "​",
            "style": "IPY_MODEL_de08f209333943038d31d7ddbf160cf9",
            "value": " 29.0/29.0 [00:00&lt;00:00, 414B/s]"
          }
        },
        "9129aabd8c6941f780ca9b064bcab931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724f205e7c9d41a181076d57a514ca42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5864f48a1208422794882c0299a7dda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5beda64b4365408080c3c4fccadfb6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2eb59ea1654c1eaf12d89c7609508c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4576d5977bde40f5bf979c450868b974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de08f209333943038d31d7ddbf160cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80317499da32421b8837a45a1be665d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53390b71dcf846838219a2a1bb8ee696",
              "IPY_MODEL_03dcb9c21a0047158d2d75da81a0b86b",
              "IPY_MODEL_729b8caa67b54e6a92ac82e2e82937fc"
            ],
            "layout": "IPY_MODEL_e9e272909fc44b65956b3f3c032793ac"
          }
        },
        "53390b71dcf846838219a2a1bb8ee696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fdc0dc8dcc94494b9277e4a1ed21910",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0fca2bf7c84030a3317c6bddce768c",
            "value": "Downloading: 100%"
          }
        },
        "03dcb9c21a0047158d2d75da81a0b86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17593b55e8184a9da8d924e4a13fa5ae",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d9a93ff23094fa292bd1a1ba5655d28",
            "value": 570
          }
        },
        "729b8caa67b54e6a92ac82e2e82937fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8423c7bda94d43a6894125a70f32dc59",
            "placeholder": "​",
            "style": "IPY_MODEL_786270c935eb487d820262c6c49cc6ae",
            "value": " 570/570 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "e9e272909fc44b65956b3f3c032793ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fdc0dc8dcc94494b9277e4a1ed21910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0fca2bf7c84030a3317c6bddce768c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17593b55e8184a9da8d924e4a13fa5ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9a93ff23094fa292bd1a1ba5655d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8423c7bda94d43a6894125a70f32dc59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786270c935eb487d820262c6c49cc6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99c199e53d954734964714f71756d668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4c2d340f7044248b92e79e62af24c31",
              "IPY_MODEL_22fce33b21c0420da56e65575bb4f362",
              "IPY_MODEL_39c26be947ca401eae3f1b4a50eba3fd"
            ],
            "layout": "IPY_MODEL_52e4473bbbfc4dec80e4cae1c8e56583"
          }
        },
        "a4c2d340f7044248b92e79e62af24c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90baea8d5ca547b5b3cd93a89f2cd38c",
            "placeholder": "​",
            "style": "IPY_MODEL_7ed3da74ba3c4f0d9abeca7ebbc978c6",
            "value": "Downloading: 100%"
          }
        },
        "22fce33b21c0420da56e65575bb4f362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a22f4a355444d1adc504b7e58661a1",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_113a8c1eb33949209afc15715710ba54",
            "value": 435779157
          }
        },
        "39c26be947ca401eae3f1b4a50eba3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560cd7a9446c4d1b91e0ec03e602fc52",
            "placeholder": "​",
            "style": "IPY_MODEL_19002c19ff9f42658a07a742202a85d4",
            "value": " 436M/436M [00:08&lt;00:00, 76.3MB/s]"
          }
        },
        "52e4473bbbfc4dec80e4cae1c8e56583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90baea8d5ca547b5b3cd93a89f2cd38c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed3da74ba3c4f0d9abeca7ebbc978c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10a22f4a355444d1adc504b7e58661a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113a8c1eb33949209afc15715710ba54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "560cd7a9446c4d1b91e0ec03e602fc52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19002c19ff9f42658a07a742202a85d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnahzeus/fintech-soc-project-2/blob/main/Project2_Fintech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q5v4zoAy9tZ",
        "outputId": "e85080e2-d28b-4adf-fe11-5467e1c85da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 36.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 39.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 36.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 36.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 430 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 604 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 778 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 860 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 952 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 37.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 37.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pKBh29GzCbN",
        "outputId": "8c3b6ea6-6abc-4b41-e4b4-1a81b1ce7d4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 16.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 54.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIM6l-zbzCfn",
        "outputId": "5226bda1-3a83-48cc-fccb-a592b76365a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.8.16\n",
            "IPython version      : 7.9.0\n",
            "\n",
            "numpy       : 1.21.6\n",
            "pandas      : 1.3.5\n",
            "torch       : 1.13.0+cu116\n",
            "transformers: 4.25.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJIYd9OYzCid",
        "outputId": "0f63f8e9-264d-49d8-a142-280c65030b70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKyRjnUhEBrw",
        "outputId": "00d7aa16-8054-4adb-d35d-b0863a575735"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/gnahzeus/fintech-soc-project-2/main/stock_data.csv?token=GHSAT0AAAAAABXSUI3RG46QXNIOD2T4CCMAY44ODJA'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eAFr1IYXzClU",
        "outputId": "94c02864-11a0-4b9d-dea3-48daf659bc9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  Sentiment\n",
              "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
              "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
              "2  user I'd be afraid to short AMZN - they are lo...          1\n",
              "3                                  MNTA Over 12.00            1\n",
              "4                                   OI  Over 21.37            1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1645be26-6dad-433f-a7ec-4b9095797192\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MNTA Over 12.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OI  Over 21.37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1645be26-6dad-433f-a7ec-4b9095797192')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1645be26-6dad-433f-a7ec-4b9095797192 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1645be26-6dad-433f-a7ec-4b9095797192');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating == 1:\n",
        "    return 1\n",
        "  elif rating == -1:\n",
        "    return 0\n",
        "\n",
        "df['sentiment'] = df.Sentiment.apply(to_sentiment)\n",
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PQuZFUxfzCn7",
        "outputId": "a4dd132d-ac41-4b71-fd5c-1a9fc69df9e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Text  Sentiment  sentiment\n",
              "5786  Industry body CII said #discoms are likely to ...         -1          0\n",
              "5787  #Gold prices slip below Rs 46,000 as #investor...         -1          0\n",
              "5788  Workers at Bajaj Auto have agreed to a 10% wag...          1          1\n",
              "5789  #Sharemarket LIVE: Sensex off day’s high, up 6...          1          1\n",
              "5790  #Sensex, #Nifty climb off day's highs, still u...          1          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9251f06c-b680-4a1d-9d0b-9523a189750b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5786</th>\n",
              "      <td>Industry body CII said #discoms are likely to ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5787</th>\n",
              "      <td>#Gold prices slip below Rs 46,000 as #investor...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5788</th>\n",
              "      <td>Workers at Bajaj Auto have agreed to a 10% wag...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5789</th>\n",
              "      <td>#Sharemarket LIVE: Sensex off day’s high, up 6...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5790</th>\n",
              "      <td>#Sensex, #Nifty climb off day's highs, still u...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9251f06c-b680-4a1d-9d0b-9523a189750b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9251f06c-b680-4a1d-9d0b-9523a189750b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9251f06c-b680-4a1d-9d0b-9523a189750b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
      ],
      "metadata": {
        "id": "oOrrQZ6yzCqz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "86cfdcc89e954677b4c26516ebc25731",
            "04d0fc28e7154c63bf4c6eb6c83ad27d",
            "e99f8f729686496697cfa443b6c01d2d",
            "20efbcadf0f54b358e3b239078d35763",
            "934b9c4ff6194f2b96b266297baf6f7e",
            "80cd35fe6f5747e0b9f61df1a9dca2dc",
            "bca132f1ca25427c87ff65b4e1275aed",
            "a6a8a65951e546b2ba3acf1c019d4a0b",
            "54afbaace4b04c0dac71b08f995f99ac",
            "0d28db5e740a4cafafd26973fb21e2d7",
            "b3e2be223069448b96b71405211c8f35",
            "b4e17e8722bc43a0bb18f82de8ab6498",
            "4c51cb983a0f4088962c53f7658a341d",
            "e00aa54e94504d1392ed08dd60aa8a63",
            "3389e5c809d7440da49233fb55452801",
            "9129aabd8c6941f780ca9b064bcab931",
            "724f205e7c9d41a181076d57a514ca42",
            "5864f48a1208422794882c0299a7dda6",
            "5beda64b4365408080c3c4fccadfb6f4",
            "3b2eb59ea1654c1eaf12d89c7609508c",
            "4576d5977bde40f5bf979c450868b974",
            "de08f209333943038d31d7ddbf160cf9",
            "80317499da32421b8837a45a1be665d7",
            "53390b71dcf846838219a2a1bb8ee696",
            "03dcb9c21a0047158d2d75da81a0b86b",
            "729b8caa67b54e6a92ac82e2e82937fc",
            "e9e272909fc44b65956b3f3c032793ac",
            "2fdc0dc8dcc94494b9277e4a1ed21910",
            "3f0fca2bf7c84030a3317c6bddce768c",
            "17593b55e8184a9da8d924e4a13fa5ae",
            "3d9a93ff23094fa292bd1a1ba5655d28",
            "8423c7bda94d43a6894125a70f32dc59",
            "786270c935eb487d820262c6c49cc6ae"
          ]
        },
        "id": "3EUeF3mRzCtb",
        "outputId": "5d95a25f-2989-4610-9945-2ff84f5aa999"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86cfdcc89e954677b4c26516ebc25731"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4e17e8722bc43a0bb18f82de8ab6498"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80317499da32421b8837a45a1be665d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_txt = 'Kickers on my watchlist XIDE TIT SOQ PNK CPW BPZ AJ  trade method 1 or method 2, see prev posts.'"
      ],
      "metadata": {
        "id": "1eDU7y22zCvv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Sentence: {sample_txt}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9EUYP3-zCyY",
        "outputId": "9f635eeb-2ba0-4f5b-90f2-db3b8a1feea6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sentence: Kickers on my watchlist XIDE TIT SOQ PNK CPW BPZ AJ  trade method 1 or method 2, see prev posts.\n",
            "   Tokens: ['Kick', '##ers', 'on', 'my', 'watch', '##list', 'XI', '##DE', 'T', '##IT', 'S', '##O', '##Q', 'P', '##N', '##K', 'CP', '##W', 'BP', '##Z', 'AJ', 'trade', 'method', '1', 'or', 'method', '2', ',', 'see', 'pre', '##v', 'posts', '.']\n",
            "Token IDs: [27596, 1468, 1113, 1139, 2824, 7276, 10252, 20427, 157, 12150, 156, 2346, 4880, 153, 2249, 2428, 20360, 2924, 21062, 5301, 22078, 2597, 3442, 122, 1137, 3442, 123, 117, 1267, 3073, 1964, 8345, 119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=64,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HL0t49UzC0a",
        "outputId": "8ea6a7de-eec0-4cb5-8e9f-52e7846e4f1d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8bTkR4LzC3S",
        "outputId": "689b2cba-ef6c-4e3b-f678-a222c9f96753"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101, 27596,  1468,  1113,  1139,  2824,  7276, 10252, 20427,   157,\n",
              "        12150,   156,  2346,  4880,   153,  2249,  2428, 20360,  2924, 21062,\n",
              "         5301, 22078,  2597,  3442,   122,  1137,  3442,   123,   117,  1267,\n",
              "         3073,  1964,  8345,   119,   102,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN8LZNxYzC5X",
        "outputId": "b07aed43-e11f-4439-ead9-7d38e2a1e6d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbxzzrbFzC7-",
        "outputId": "bb81d70a-0070-4e43-b2a0-cc0972fef1d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'Kick',\n",
              " '##ers',\n",
              " 'on',\n",
              " 'my',\n",
              " 'watch',\n",
              " '##list',\n",
              " 'XI',\n",
              " '##DE',\n",
              " 'T',\n",
              " '##IT',\n",
              " 'S',\n",
              " '##O',\n",
              " '##Q',\n",
              " 'P',\n",
              " '##N',\n",
              " '##K',\n",
              " 'CP',\n",
              " '##W',\n",
              " 'BP',\n",
              " '##Z',\n",
              " 'AJ',\n",
              " 'trade',\n",
              " 'method',\n",
              " '1',\n",
              " 'or',\n",
              " 'method',\n",
              " '2',\n",
              " ',',\n",
              " 'see',\n",
              " 'pre',\n",
              " '##v',\n",
              " 'posts',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df.Text:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "metadata": {
        "id": "cfmUS39xzC-k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "cS3k_IZYzDDr",
        "outputId": "66b0a782-a707-49f8-f5e8-5a040ddbfd07"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 400,
              "height": 261
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 70"
      ],
      "metadata": {
        "id": "t3sJbyDvzDGb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "ElJqEflCzDJE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "pOOxLuGmzDL3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYASqUebzDOu",
        "outputId": "5cb4c9e5-653d-4abc-aee6-3ef637ec0777"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5211, 3), (290, 3), (290, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.Text.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "Rsh7L4U8zDRU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqv00JtzzDUN",
        "outputId": "838e370c-92b9-48b9-da97-dd6d5fd84f45"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le66dBe50sX8",
        "outputId": "069bf3e3-0523-47d5-bc28-4bed2f99ae21"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 70])\n",
            "torch.Size([16, 70])\n",
            "torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15MnHOKY0scg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "99c199e53d954734964714f71756d668",
            "a4c2d340f7044248b92e79e62af24c31",
            "22fce33b21c0420da56e65575bb4f362",
            "39c26be947ca401eae3f1b4a50eba3fd",
            "52e4473bbbfc4dec80e4cae1c8e56583",
            "90baea8d5ca547b5b3cd93a89f2cd38c",
            "7ed3da74ba3c4f0d9abeca7ebbc978c6",
            "10a22f4a355444d1adc504b7e58661a1",
            "113a8c1eb33949209afc15715710ba54",
            "560cd7a9446c4d1b91e0ec03e602fc52",
            "19002c19ff9f42658a07a742202a85d4"
          ]
        },
        "id": "GHf_ICdE0sdG",
        "outputId": "c0fed7cc-b87c-4099-9e6c-b4877d80449f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99c199e53d954734964714f71756d668"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask'],\n",
        "  return_dict=False\n",
        ")"
      ],
      "metadata": {
        "id": "w5MKqZrO0sfs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LUUf-6I0siX",
        "outputId": "cefede3f-4f54-4070-9b83-be0ff25f0bc8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9nA93000sk7",
        "outputId": "c72ad4cf-debc-491d-9a02-e5c0fc71440d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "ZI9lDs3w1Fpp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentClassifier(2)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCqCeWRe1Fsf",
        "outputId": "c84b846f-9a92-4a00-d963-dd4fca85c06c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5uksy1v1Fvn",
        "outputId": "54be4219-424d-4086-a03e-5bf78837c6cd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 70])\n",
            "torch.Size([16, 70])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIyMNpg_1FyP",
        "outputId": "9599b99b-1c26-412c-d14b-3ad8c0ec7094"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3002, 0.6998],\n",
              "        [0.4420, 0.5580],\n",
              "        [0.6270, 0.3730],\n",
              "        [0.3465, 0.6535],\n",
              "        [0.7202, 0.2798],\n",
              "        [0.3440, 0.6560],\n",
              "        [0.4927, 0.5073],\n",
              "        [0.6337, 0.3663],\n",
              "        [0.6319, 0.3681],\n",
              "        [0.4927, 0.5073],\n",
              "        [0.5879, 0.4121],\n",
              "        [0.6320, 0.3680],\n",
              "        [0.3564, 0.6436],\n",
              "        [0.5387, 0.4613],\n",
              "        [0.5396, 0.4604],\n",
              "        [0.2882, 0.7118]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-M_wy1y1F1F",
        "outputId": "4822c20d-d4c9-4e14-ec19-35b5f41d3722"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "YBeBE0Kp1F39"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "o7rQF1HB0sni"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ00d9Jl0sqO",
        "outputId": "6e5a645a-0541-436b-a73e-b8c7a9ee1a8d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5739796956989662 accuracy 0.6996737670312799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.4115648893149276 accuracy 0.7965517241379311\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.32906443976505767 accuracy 0.8599117251966992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5277163450066981 accuracy 0.8068965517241379\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1476679341499751 accuracy 0.9554787948570331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8340693160916042 accuracy 0.8206896551724138\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.07139972323338227 accuracy 0.9831126463250816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1661827731264853 accuracy 0.8172413793103448\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.042927506156058615 accuracy 0.990980617923623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2761513141204457 accuracy 0.8068965517241379\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02496774880380864 accuracy 0.9942429476108232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2971437199923552 accuracy 0.8241379310344827\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01747729112752149 accuracy 0.996353866820188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4380111039469117 accuracy 0.8103448275862069\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00919959266305179 accuracy 0.9976971790443292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5394845577447038 accuracy 0.8103448275862069\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.004443473836573207 accuracy 0.9986566877758587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5593334640327252 accuracy 0.8137931034482758\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.005491626457131501 accuracy 0.9984647860295528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6127887166252262 accuracy 0.8172413793103448\n",
            "\n",
            "CPU times: user 8min 50s, sys: 3min 40s, total: 12min 31s\n",
            "Wall time: 12min 44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzD1fr910stB",
        "outputId": "8c294334-1c9d-4ed6-837c-e5bf00d4bfbc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8413793103448276"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "metadata": {
        "id": "Gh_KD5k_M6XT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hijYVb92M7Xo",
        "outputId": "5ed38120-5e77-490d-d162-65225d230f33"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=[\"negative\", \"positive\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Who1qh4M_uy",
        "outputId": "fc639f6a-19d4-4e7d-b9d3-af5db329d68b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.78      0.79       108\n",
            "    positive       0.87      0.88      0.87       182\n",
            "\n",
            "    accuracy                           0.84       290\n",
            "   macro avg       0.83      0.83      0.83       290\n",
            "weighted avg       0.84      0.84      0.84       290\n",
            "\n"
          ]
        }
      ]
    }
  ]
}